################################### 
#            configuration        #
###################################
[DEFAULT]
task_name = legal
output_dir=tmp/%(task_name)s
use_entity_indicator=True

###超参设置
batch_size = 32
num_epoch = 30

; max_src_len = 512  #文本的最大句子长度
max_seq_len=512
 #解码序列的最大长度为10
max_trg_len = 10  
embedding_file = os.path.join(src_data_folder, 'w2v.txt')
update_freq = 1
wf = 1.0
att_type = 2

# bool(int(sys.argv[13]))
use_hadamard = False  
gcn_num_layers = 3
# enc_type = ['LSTM', 'GCN', 'LSTM-GCN'][0]   #默认是用LSTM

word_embed_dim = 300
word_min_freq = 2

char_embed_dim = 50
char_feature_size = 50
conv_filter_size = 3
#词的最大长度为10个字符
max_word_len = 10  
# word_embed_dim
positional_embed_dim = 300 
max_positional_idx = 100

# word_embed_dim + char_feature_size
enc_inp_size = 350  
# 编码器隐层向量维度
enc_hidden_size = 768
 # enc_hidden_size
dec_inp_size = 768
#dec_inp_size
dec_hidden_size = 768
l1_type_embed_dim = 50

dec_att_type=2

#relation个数加1
rel_size=6 

drop_rate = 0.3
layers = 2
early_stop_cnt = 10
[MODEL]
seed = 107
model_path = ../backup/Robera/
; bert-base-uncased
[Train]
num_train_epochs=30
# Total number of training epochs to perform.
learning_rate=2e-5 
# The initial learning rate for Adam.
per_gpu_train_batch_size=2
# Batch size per GPU/CPU for training.
test_batch_size=16
# Batch size per GPU/CPU for evaluation.  
no_cuda=False
# Avoid using CUDA when available

[Dataset]
data_dir= ../data 

train=True
eval=True
evaluate_during_training=True

gradient_accumulation_steps=1
# Number of updates steps to accumulate before performing a backward/update pass.

weight_decay=0.0
# Weight deay if we apply some.
adam_epsilon=1e-8
# Epsilon for Adam optimizer.
max_grad_norm=1.0
# Max gradient norm.

max_steps=-1
# If > 0: set total number of training steps to perform. Override num_train_epochs.
warmup_steps=1000

# Linear warmup over warmup_steps.
logging_steps=50
# Log every X updates steps.

save_steps=1000
# help="Save checkpoint every X updates steps.

eval_all_checkpoints=False
# Evaluate all checkpoints starting with the same prefix as model_name ending and ending with step number
#l2_reg_lambda=5e-3
l2_reg_lambda=0

overwrite_output_dir=True
# Overwrite the content of the output directory
overwrite_cache=True
# Overwrite the cached training and evaluation sets
local_rank=-1
# For distributed training: local_rank